{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 (50 points) \n",
    "\n",
    "Vapor-liquid equilibria data are correlated using two adjustable parameters $A_{12}$ and $A_{21}$ per binary\n",
    "mixture. For low pressures, the equilibrium relation can be formulated as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p = & x_1\\exp\\left(A_{12}\\left(\\frac{A_{21}x_2}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{water}^{sat}\\\\\n",
    "& + x_2\\exp\\left(A_{21}\\left(\\frac{A_{12}x_1}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{1,4 dioxane}^{sat}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here the saturation pressures are given by the Antoine equation\n",
    "\n",
    "$$\n",
    "\\log_{10}(p^{sat}) = a_1 - \\frac{a_2}{T + a_3},\n",
    "$$\n",
    "\n",
    "where $T = 20$($^{\\circ}{\\rm C}$) and $a_{1,2,3}$ for a water - 1,4 dioxane\n",
    "system is given below.\n",
    "\n",
    "|             | $a_1$     | $a_2$      | $a_3$     |\n",
    "|:------------|:--------|:---------|:--------|\n",
    "| Water       | 8.07131 | 1730.63  | 233.426 |\n",
    "| 1,4 dioxane | 7.43155 | 1554.679 | 240.337 |\n",
    "\n",
    "\n",
    "The following table lists the measured data. Recall that in a binary system $x_1 + x_2 = 1$.\n",
    "\n",
    "|$x_1$ | 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0 |\n",
    "|:-----|:--------|:---------|:--------|:-----|:-----|:-----|:-----|:-----|:-----|:-----|:-----|\n",
    "|$p$| 28.1 | 34.4 | 36.7 | 36.9 | 36.8 | 36.7 | 36.5 | 35.4 | 32.9 | 27.7 | 17.5 |\n",
    "\n",
    "Estimate $A_{12}$ and $A_{21}$ using data from the above table: \n",
    "\n",
    "1. Formulate the least square problem; \n",
    "2. Since the model is nonlinear, the problem does not have an analytical solution. Therefore, solve it using the gradient descent or Newton's method implemented in HW1; \n",
    "3. Compare your optimized model with the data. Does your model fit well with the data?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to Problem 1 \n",
    "---\n",
    "\n",
    "#### Part I \n",
    "---\n",
    "\n",
    "We want to find $A_{12}$ and $A_{21}$ such that the function $p$ is able to approximate the equlibrial pressure for two given liquid (water and 1,4-dioxane). \n",
    "\n",
    "The optimization problem is to find parameters $A_{12}$ and $A_{21}$ such that the function $P\\;(X; A_{12}, A_{21})$ approaches the measured data provided in the problem. The objective function can be formulated as the least square sum of the difference between the predicted and measured data, as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text{minimize:} && \\sum_{i=1}^{N = 11} (P\\;(X_i; A_{12}, A_{21}) - P_i)^2  \\\\\n",
    "&A_{12}, A_{21}\n",
    "\\end{aligned}\n",
    "$$\n",
    "where, \n",
    "\n",
    "$X = (x_1, x_2)$\n",
    "\n",
    "---\n",
    "#### Part II\n",
    "---\n",
    "\n",
    "Solving using gradient descent. Gradient descent with step size ($\\alpha = 0.001$) is picked and the gradient is calculated through back propagation using PyTorch's autograd feature. The code below can be executed to obtain the optimized parameter $A$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.9584178 1.6891867]\n",
      "[[0.67019307]]\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Data Points\n",
    "xi = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "# Binary, calculate x2 from x1\n",
    "var = lambda x :  [x, 1-x]\n",
    "\n",
    "# Define a variable, make sure requires_grad=True so that PyTorch can take gradient with respect to this variable\n",
    "# make (x1, x2) from the given data points\n",
    "x = t.from_numpy(np.array(list((map(var, xi)))))\n",
    "\n",
    "# Define variable to optimize\n",
    "a = Variable(t.tensor([1.0 , 1.0 ]), requires_grad=True)\n",
    "\n",
    "# calculate psat values for water and 1,4-dioxane\n",
    "def psat(liq):\n",
    "    if liq == \"water\":\n",
    "        a1 = 8.07131\n",
    "        a2 = 1730.63\n",
    "        a3 = 233.426\n",
    "    else:\n",
    "        a1 = 7.43155\n",
    "        a2 = 1554.679\n",
    "        a3 = 240.337\n",
    "    \n",
    "    return (10**(a1 - (a2/(20+a3))))\n",
    "\n",
    "# Define loss as least square sum \n",
    "def lse_loss(x, y, a):\n",
    "    A = a.clone()\n",
    "    pi = lambda x, a: x[0]*t.exp(a[0]*((a[1]*x[1])/(a[0]*x[0]+a[1]*x[1]))**2)*psat(\"water\") \\\n",
    "        + x[1]*t.exp(a[1]*((a[0]*x[0])/(a[0]*x[0]+a[1]*x[1]))**2)*psat(\"dioxane\") \n",
    "    \n",
    "    # stack the estimated value\n",
    "    p = t.stack([pi(x[i], A) for i in range(x.size()[0])])\n",
    "    \n",
    "    return t.mm((p - y).reshape(1, -1), \n",
    "                (p - y).reshape(1, -1).t())\n",
    "\n",
    "\n",
    "def lin_search(f, grad, x, a):\n",
    "    tilt = 0.8\n",
    "    alpha = 1\n",
    "    d = -grad\n",
    "    x = x[0]\n",
    "    while f(x, a+alpha*d) > (f(x,a)+tilt*alpha*np.matmul(grad.t(), d)):\n",
    "        alpha *= tilt\n",
    "    return alpha\n",
    "    \n",
    "\n",
    "# measured data : y\n",
    "y = t.tensor([28.1, 34.4, 36.7, 36.9, 36.8, 36.7, 36.5, 35.4, 32.9, 27.7, 17.5])\n",
    "\n",
    "\n",
    "# Equlibrium pressure formula\n",
    "pi = lambda x, a: x[0]*t.exp(a[0]*((a[1]*x[1])/(a[0]*x[0]+a[1]*x[1]))**2)*psat(\"water\") \\\n",
    "        + x[1]*t.exp(a[1]*((a[0]*x[0])/(a[0]*x[0]+a[1]*x[1]))**2)*psat(\"dioxane\") \n",
    "\n",
    "# Fix the step size\n",
    "alpha = 0.001\n",
    "\n",
    "\n",
    "# Start gradient descent\n",
    "for i in range(100):  # TODO: change the termination criterion\n",
    "    loss = lse_loss(x, y, a)\n",
    "    \n",
    "    #loss = a[0]\n",
    "    loss.backward(retain_graph=True)\n",
    "    \n",
    "    # no_grad() specifies that the operations within this context are not part of the computational graph, i.e., we don't need the gradient descent algorithm itself to be differentiable with respect to x\n",
    "    with t.no_grad():\n",
    "        #t.autograd.set_detect_anomaly(True)\n",
    "        #alpha = lin_search(pi, a.grad, x, a)\n",
    "        a -= alpha * a.grad\n",
    "        # need to clear the gradient at every step, or otherwise it will accumulate...\n",
    "        #print(loss.data.numpy())\n",
    "        a.grad.zero_()\n",
    "            \n",
    "print(a.data.numpy())\n",
    "print(loss.data.numpy())\n",
    "\n",
    "\n",
    "# Check the gradient. numpy() turns the variable from a PyTorch tensor to a numpy array.\n",
    "#a.grad.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**The value of $A$ was found to be:**\n",
    "\n",
    "$[A_{12}, A_{21}] = [1.9584178, \\;\\; 1.6891867]$ \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "with the loss of $0.67019307$. \n",
    "\n",
    "The gradient descent algorithm converged to this particular local minimum and couldn't update further. \n",
    "\n",
    "\n",
    "#### Part III\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value with A = [1.9584178 1.6891867]\n",
      "------\n",
      "[28.82409953 34.64431155 36.45296627 36.86731277 36.87400692 36.74983378\n",
      " 36.39044727 35.38482398 32.94778604 27.73000205 17.47325208]\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimated Value with A =\", a.data.numpy())\n",
    "\n",
    "print ('------')\n",
    "print(t.stack([pi(x[i], a) for i in range(x.size()[0])]).detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Estimated value is determined and compared below (rounded to nearest ten):\n",
    "\n",
    "|$x_1$ | 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0 |\n",
    "|:-----|:--------|:---------|:--------|:-----|:-----|:-----|:-----|:-----|:-----|:-----|:-----|\n",
    "|$p$| 28.1 | 34.4 | 36.7 | 36.9 | 36.8 | 36.7 | 36.5 | 35.4 | 32.9 | 27.7 | 17.5 |\n",
    "|$p_{est}$| 28.8 | 34.6 | 36.5 | 36.9 | 36.9 | 36.8 | 36.4 | 35.4 | 33.0 | 27.7 | 17.5 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Model has fit the data well with the squared error of about $0.67$. The model could perform even better if we provide more data. With the data fed to the algorithm, there is a high probability that the model has overfit the data. To avoid overfitting, feeding huge datasets usually helps among several other options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Problem 2 (50 points) \n",
    "\n",
    "Solve the following problem using Bayesian Optimization:\n",
    "$$\n",
    "    \\min_{x_1, x_2} \\quad \\left(4-2.1x_1^2 + \\frac{x_1^4}{3}\\right)x_1^2 + x_1x_2 + \\left(-4 + 4x_2^2\\right)x_2^2,\n",
    "$$\n",
    "for $x_1 \\in [-3,3]$ and $x_2 \\in [-2,2]$. A tutorial on Bayesian Optimization can be found [here](https://thuijskens.github.io/2016/12/29/bayesian-optimisation/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution to Problem 2\n",
    "---\n",
    "\n",
    "The bayesian optimization library provided in the tutorial above is used to solve the optimization problem. Bayesian optimization works by sampling an unknown function at several different points in the space such that the points are \"spread-out\" over the space. As the iteration increases, this optimization process finds the region of space that has minimum functional value compared to the the previous position. It is an heuristic process and does not guarantee convergence, however, it can be used to \"improve\" the current best solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Below is the code borrowed from the [tutorial](https://thuijskens.github.io/2016/12/29/bayesian-optimisation/).**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" gp.py\n",
    "Bayesian optimisation of loss functions.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.gaussian_process as gp\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def expected_improvement(x, gaussian_process, evaluated_loss, greater_is_better=False, n_params=1):\n",
    "    \"\"\" expected_improvement\n",
    "    Expected improvement acquisition function.\n",
    "    Arguments:\n",
    "    ----------\n",
    "        x: array-like, shape = [n_samples, n_hyperparams]\n",
    "            The point for which the expected improvement needs to be computed.\n",
    "        gaussian_process: GaussianProcessRegressor object.\n",
    "            Gaussian process trained on previously evaluated hyperparameters.\n",
    "        evaluated_loss: Numpy array.\n",
    "            Numpy array that contains the values off the loss function for the previously\n",
    "            evaluated hyperparameters.\n",
    "        greater_is_better: Boolean.\n",
    "            Boolean flag that indicates whether the loss function is to be maximised or minimised.\n",
    "        n_params: int.\n",
    "            Dimension of the hyperparameter space.\n",
    "    \"\"\"\n",
    "\n",
    "    x_to_predict = x.reshape(-1, n_params)\n",
    "\n",
    "    mu, sigma = gaussian_process.predict(x_to_predict, return_std=True)\n",
    "\n",
    "    if greater_is_better:\n",
    "        loss_optimum = np.max(evaluated_loss)\n",
    "    else:\n",
    "        loss_optimum = np.min(evaluated_loss)\n",
    "\n",
    "    scaling_factor = (-1) ** (not greater_is_better)\n",
    "\n",
    "    # In case sigma equals zero\n",
    "    with np.errstate(divide='ignore'):\n",
    "        Z = scaling_factor * (mu - loss_optimum) / sigma\n",
    "        expected_improvement = scaling_factor * (mu - loss_optimum) * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        expected_improvement[sigma == 0.0] == 0.0\n",
    "\n",
    "    return -1 * expected_improvement\n",
    "\n",
    "\n",
    "def sample_next_hyperparameter(acquisition_func, gaussian_process, evaluated_loss, greater_is_better=False,\n",
    "                               bounds=(0, 10), n_restarts=25):\n",
    "    \"\"\" sample_next_hyperparameter\n",
    "    Proposes the next hyperparameter to sample the loss function for.\n",
    "    Arguments:\n",
    "    ----------\n",
    "        acquisition_func: function.\n",
    "            Acquisition function to optimise.\n",
    "        gaussian_process: GaussianProcessRegressor object.\n",
    "            Gaussian process trained on previously evaluated hyperparameters.\n",
    "        evaluated_loss: array-like, shape = [n_obs,]\n",
    "            Numpy array that contains the values off the loss function for the previously\n",
    "            evaluated hyperparameters.\n",
    "        greater_is_better: Boolean.\n",
    "            Boolean flag that indicates whether the loss function is to be maximised or minimised.\n",
    "        bounds: Tuple.\n",
    "            Bounds for the L-BFGS optimiser.\n",
    "        n_restarts: integer.\n",
    "            Number of times to run the minimiser with different starting points.\n",
    "    \"\"\"\n",
    "    best_x = None\n",
    "    best_acquisition_value = 1\n",
    "    n_params = bounds.shape[0]\n",
    "\n",
    "    for starting_point in np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_restarts, n_params)):\n",
    "\n",
    "        res = minimize(fun=acquisition_func,\n",
    "                       x0=starting_point.reshape(1, -1),\n",
    "                       bounds=bounds,\n",
    "                       method='L-BFGS-B',\n",
    "                       args=(gaussian_process, evaluated_loss, greater_is_better, n_params))\n",
    "\n",
    "        if res.fun < best_acquisition_value:\n",
    "            best_acquisition_value = res.fun\n",
    "            best_x = res.x\n",
    "\n",
    "    return best_x\n",
    "\n",
    "\n",
    "def bayesian_optimisation(n_iters, sample_loss, bounds, x0=None, n_pre_samples=5,\n",
    "                          gp_params=None, random_search=False, alpha=1e-5, epsilon=1e-7):\n",
    "    \"\"\" bayesian_optimisation\n",
    "    Uses Gaussian Processes to optimise the loss function `sample_loss`.\n",
    "    Arguments:\n",
    "    ----------\n",
    "        n_iters: integer.\n",
    "            Number of iterations to run the search algorithm.\n",
    "        sample_loss: function.\n",
    "            Function to be optimised.\n",
    "        bounds: array-like, shape = [n_params, 2].\n",
    "            Lower and upper bounds on the parameters of the function `sample_loss`.\n",
    "        x0: array-like, shape = [n_pre_samples, n_params].\n",
    "            Array of initial points to sample the loss function for. If None, randomly\n",
    "            samples from the loss function.\n",
    "        n_pre_samples: integer.\n",
    "            If x0 is None, samples `n_pre_samples` initial points from the loss function.\n",
    "        gp_params: dictionary.\n",
    "            Dictionary of parameters to pass on to the underlying Gaussian Process.\n",
    "        random_search: integer.\n",
    "            Flag that indicates whether to perform random search or L-BFGS-B optimisation\n",
    "            over the acquisition function.\n",
    "        alpha: double.\n",
    "            Variance of the error term of the GP.\n",
    "        epsilon: double.\n",
    "            Precision tolerance for floats.\n",
    "    \"\"\"\n",
    "\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "\n",
    "    n_params = bounds.shape[0]\n",
    "\n",
    "    if x0 is None:\n",
    "        for params in np.random.uniform(bounds[:, 0], bounds[:, 1], (n_pre_samples, bounds.shape[0])):\n",
    "            x_list.append(params)\n",
    "            y_list.append(sample_loss(params))\n",
    "    else:\n",
    "        for params in x0:\n",
    "            x_list.append(params)\n",
    "            y_list.append(sample_loss(params))\n",
    "\n",
    "    xp = np.array(x_list)\n",
    "    yp = np.array(y_list)\n",
    "\n",
    "    # Create the GP\n",
    "    if gp_params is not None:\n",
    "        model = gp.GaussianProcessRegressor(**gp_params)\n",
    "    else:\n",
    "        kernel = gp.kernels.Matern()\n",
    "        model = gp.GaussianProcessRegressor(kernel=kernel,\n",
    "                                            alpha=alpha,\n",
    "                                            n_restarts_optimizer=10,\n",
    "                                            normalize_y=True)\n",
    "\n",
    "    for n in range(n_iters):\n",
    "\n",
    "        model.fit(xp, yp)\n",
    "\n",
    "        # Sample next hyperparameter\n",
    "        if random_search:\n",
    "            x_random = np.random.uniform(bounds[:, 0], bounds[:, 1], size=(random_search, n_params))\n",
    "            ei = -1 * expected_improvement(x_random, model, yp, greater_is_better=True, n_params=n_params)\n",
    "            next_sample = x_random[np.argmax(ei), :]\n",
    "        else:\n",
    "            next_sample = sample_next_hyperparameter(expected_improvement, model, yp, greater_is_better=True, bounds=bounds, n_restarts=100)\n",
    "\n",
    "        # Duplicates will break the GP. In case of a duplicate, we will randomly sample a next query point.\n",
    "        if np.any(np.abs(next_sample - xp) <= epsilon):\n",
    "            next_sample = np.random.uniform(bounds[:, 0], bounds[:, 1], bounds.shape[0])\n",
    "\n",
    "        # Sample loss for new set of parameters\n",
    "        cv_score = sample_loss(next_sample)\n",
    "\n",
    "        # Update lists\n",
    "        x_list.append(next_sample)\n",
    "        y_list.append(cv_score)\n",
    "\n",
    "        # Update xp and yp\n",
    "        xp = np.array(x_list)\n",
    "        yp = np.array(y_list)\n",
    "\n",
    "    return xp, yp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the objective function and its bounds as provided in the problem statement. Since, bayesian optimization process stores all the functional values sampled until the last iteration, we can query the minimum value and the index of the minimum value to find the parameter corresponding minimum value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def obj_f(x):\n",
    "    return (4 - 2.1*x[0]**2 + (x[0]**4)/3)*x[0]**2 + x[0]*x[1] + (4*x[1]**2 - 4)*x[1]**2\n",
    "\n",
    "bounds = np.array([[-3, 3],[-2,2]])\n",
    "xp, yp = bayesian_optimisation(n_iters=47, \n",
    "                               sample_loss=obj_f, \n",
    "                               bounds=bounds,\n",
    "                               n_pre_samples=3,\n",
    "                               random_search=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.09697612, -0.60445253],\n",
       "        [-2.02608699, -1.90457813],\n",
       "        [-1.38745031, -0.31171466],\n",
       "        [-2.20171154, -1.99555376],\n",
       "        [-2.50819131, -1.99943173],\n",
       "        [-2.9577226 , -1.99912119],\n",
       "        [-2.99860898, -1.76437286],\n",
       "        [-2.99690928, -1.98490992],\n",
       "        [ 2.98673774,  1.9859951 ],\n",
       "        [ 2.99708787,  1.36259697],\n",
       "        [ 2.36085574,  1.99809378],\n",
       "        [ 2.99938986, -1.976032  ],\n",
       "        [ 2.99555897, -1.31206416],\n",
       "        [ 2.37867907, -1.99701792],\n",
       "        [ 2.99733345,  1.8110451 ],\n",
       "        [-2.9746949 ,  1.99944102],\n",
       "        [-2.31230115,  1.99796823],\n",
       "        [-2.99735226,  1.44101137],\n",
       "        [ 2.99442698,  0.19212151],\n",
       "        [ 0.09689871,  1.99734812],\n",
       "        [ 2.00850505,  0.2604518 ],\n",
       "        [-2.99903187,  0.14998653],\n",
       "        [-2.99860789, -0.74008508],\n",
       "        [ 0.57403237, -1.99723374],\n",
       "        [ 2.99579278, -0.53075351],\n",
       "        [-2.99535431,  0.79064467],\n",
       "        [ 2.99765298, -1.76141096],\n",
       "        [-1.02902807,  1.31393065],\n",
       "        [-0.67920401, -1.99944018],\n",
       "        [-2.99800566, -1.99872522],\n",
       "        [-2.99815495, -1.24086827],\n",
       "        [ 2.99807172,  0.80158671],\n",
       "        [ 1.10005459,  1.99705177],\n",
       "        [-2.37649337,  0.44112599],\n",
       "        [ 2.88346729,  1.99399653],\n",
       "        [ 0.52803967,  0.88412047],\n",
       "        [ 2.22799579, -0.93233159],\n",
       "        [-2.99797471,  1.81537722],\n",
       "        [-2.99697763, -0.3121379 ],\n",
       "        [-2.39319731, -0.84733189],\n",
       "        [ 1.44988572, -1.99717306],\n",
       "        [ 2.99867179,  1.99278521],\n",
       "        [-0.94753395,  1.99957271],\n",
       "        [ 2.37052594,  1.14392682],\n",
       "        [-2.99752573, -1.99762313],\n",
       "        [-0.83960617, -1.19999721],\n",
       "        [ 1.01289719, -1.11123858],\n",
       "        [-0.47606271,  0.35793054],\n",
       "        [ 2.83367101, -1.99553073],\n",
       "        [ 2.99998758,  1.99420079]]),\n",
       " array([ -0.94867681,  46.07258025,   2.37754601,  59.91063942,\n",
       "         77.99701629, 151.2583183 , 140.1086181 , 160.30687398,\n",
       "        157.57681582, 118.52382875,  67.27703666, 148.16842388,\n",
       "        108.68612016,  58.69918283, 143.48626535, 143.91008716,\n",
       "         55.45602851, 112.77459371, 107.76397507,  47.93454911,\n",
       "          4.11483562, 108.08832908, 109.73466311,  47.64636831,\n",
       "        105.31432921, 104.28506433, 129.05030338,   5.94099567,\n",
       "         50.72644399, 162.1859414 , 115.42315538, 109.83949431,\n",
       "         52.22341922,  13.98044956, 132.75550469,   0.74346748,\n",
       "          6.35026702, 133.14658622, 108.63048186,  17.86637338,\n",
       "         47.01355488, 161.69680199,  48.19724651,  19.64044222,\n",
       "        161.9233291 ,   5.43482866,   2.28782102,   0.18535784,\n",
       "        111.13986034, 162.23263668]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xp, yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val = yp.min()\n",
    "min_index = yp.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The solution is: (0.09697611920343263, -0.6044525342204645)\n",
      "And, the loss is  -0.9486768101858796\n"
     ]
    }
   ],
   "source": [
    "print(\"The solution is: (\"+ str(xp[min_index][0]) + \", \" + str(xp[min_index][1])+\")\" )\n",
    "print(\"And, the loss is \", min_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**NOTE**\n",
    "\n",
    "It is to be noted that, this process produces different result for different runs. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x14c0fe14cc0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-3,3,100)\n",
    "y = np.linspace(-2,2,100)\n",
    "\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "z_params = [[x1, x2] for x1 in x for x2 in y]\n",
    "\n",
    "Z = [obj_f(params) for params in z_params] \n",
    "\n",
    "\n",
    "p = plt.contourf(X, Y, np.array(Z).reshape(X.shape))\n",
    "plt.scatter(xp[min_index][0], xp[min_index][1], marker='x', color='yellow')\n",
    "plt.colorbar(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the objective function in the plot above and locate the point found using the Bayesian optimization. We see that the point changes with every run and iteration as the process is stochastic. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
