{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 (50 points) \n",
    "\n",
    "Vapor-liquid equilibria data are correlated using two adjustable parameters $A_{12}$ and $A_{21}$ per binary\n",
    "mixture. For low pressures, the equilibrium relation can be formulated as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p = & x_1\\exp\\left(A_{12}\\left(\\frac{A_{21}x_2}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{water}^{sat}\\\\\n",
    "& + x_2\\exp\\left(A_{21}\\left(\\frac{A_{12}x_1}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{1,4 dioxane}^{sat}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here the saturation pressures are given by the Antoine equation\n",
    "\n",
    "$$\n",
    "\\log_{10}(p^{sat}) = a_1 - \\frac{a_2}{T + a_3},\n",
    "$$\n",
    "\n",
    "where $T = 20$($^{\\circ}{\\rm C}$) and $a_{1,2,3}$ for a water - 1,4 dioxane\n",
    "system is given below.\n",
    "\n",
    "|             | $a_1$     | $a_2$      | $a_3$     |\n",
    "|:------------|:--------|:---------|:--------|\n",
    "| Water       | 8.07131 | 1730.63  | 233.426 |\n",
    "| 1,4 dioxane | 7.43155 | 1554.679 | 240.337 |\n",
    "\n",
    "\n",
    "The following table lists the measured data. Recall that in a binary system $x_1 + x_2 = 1$.\n",
    "\n",
    "|$x_1$ | 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0 |\n",
    "|:-----|:--------|:---------|:--------|:-----|:-----|:-----|:-----|:-----|:-----|:-----|:-----|\n",
    "|$p$| 28.1 | 34.4 | 36.7 | 36.9 | 36.8 | 36.7 | 36.5 | 35.4 | 32.9 | 27.7 | 17.5 |\n",
    "\n",
    "Estimate $A_{12}$ and $A_{21}$ using data from the above table: \n",
    "\n",
    "1. Formulate the least square problem; \n",
    "2. Since the model is nonlinear, the problem does not have an analytical solution. Therefore, solve it using the gradient descent or Newton's method implemented in HW1; \n",
    "3. Compare your optimized model with the data. Does your model fit well with the data?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to Problem 1 \n",
    "---\n",
    "\n",
    "#### Part I \n",
    "---\n",
    "\n",
    "We want to find $A_{12}$ and $A_{21}$ such that the function $p$ is able to approximate the equlibrial pressure for two given liquid (water and 1,4-dioxane). \n",
    "\n",
    "The optimization problem is to find parameters $A_{12}$ and $A_{21}$ such that the function $P\\;(X; A_{12}, A_{21})$ approaches the measured data provided in the problem. The objective function can be formulated as the least square sum of the difference between the predicted and measured data, as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text{minimize:} && \\sum_{i=1}^{N = 11} (P\\;(X_i; A_{12}, A_{21}) - P_i)^2  \\\\\n",
    "&A_{12}, A_{21}\n",
    "\\end{aligned}\n",
    "$$\n",
    "where, \n",
    "\n",
    "$X = (x_1, x_2)$\n",
    "\n",
    "---\n",
    "#### Part II\n",
    "---\n",
    "\n",
    "Solving using gradient descent. Gradient descent with step size ($\\alpha = 0.001$) is picked and the gradient is calculated through back propagation using PyTorch's autograd feature. The code below can be executed to obtain the optimized parameter $A$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.9584178 1.6891867]\n",
      "[[0.67019307]]\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Data Points\n",
    "xi = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "# Binary, calculate x2 from x1\n",
    "var = lambda x :  [x, 1-x]\n",
    "\n",
    "# Define a variable, make sure requires_grad=True so that PyTorch can take gradient with respect to this variable\n",
    "# make (x1, x2) from the given data points\n",
    "x = t.from_numpy(np.array(list((map(var, xi)))))\n",
    "\n",
    "# Define variable to optimize\n",
    "a = Variable(t.tensor([1.0 , 1.0 ]), requires_grad=True)\n",
    "\n",
    "# calculate psat values for water and 1,4-dioxane\n",
    "def psat(liq):\n",
    "    if liq == \"water\":\n",
    "        a1 = 8.07131\n",
    "        a2 = 1730.63\n",
    "        a3 = 233.426\n",
    "    else:\n",
    "        a1 = 7.43155\n",
    "        a2 = 1554.679\n",
    "        a3 = 240.337\n",
    "    \n",
    "    return (10**(a1 - (a2/(20+a3))))\n",
    "\n",
    "# Define loss as least square sum \n",
    "def lse_loss(x, y, a):\n",
    "    A = a.clone()\n",
    "    pi = lambda x, a: x[0]*t.exp(a[0]*((a[1]*x[1])/(a[0]*x[0]+a[1]*x[1]))**2)*psat(\"water\") \\\n",
    "        + x[1]*t.exp(a[1]*((a[0]*x[0])/(a[0]*x[0]+a[1]*x[1]))**2)*psat(\"dioxane\") \n",
    "    \n",
    "    # stack the estimated value\n",
    "    p = t.stack([pi(x[i], A) for i in range(x.size()[0])])\n",
    "    \n",
    "    return t.mm((p - y).reshape(1, -1), \n",
    "                (p - y).reshape(1, -1).t())\n",
    "\n",
    "\n",
    "def lin_search(f, grad, x, a):\n",
    "    tilt = 0.8\n",
    "    alpha = 1\n",
    "    d = -grad\n",
    "    x = x[0]\n",
    "    while f(x, a+alpha*d) > (f(x,a)+tilt*alpha*np.matmul(grad.t(), d)):\n",
    "        alpha *= tilt\n",
    "    return alpha\n",
    "    \n",
    "\n",
    "# measured data : y\n",
    "y = t.tensor([28.1, 34.4, 36.7, 36.9, 36.8, 36.7, 36.5, 35.4, 32.9, 27.7, 17.5])\n",
    "\n",
    "\n",
    "# Equlibrium pressure formula\n",
    "pi = lambda x, a: x[0]*t.exp(a[0]*((a[1]*x[1])/(a[0]*x[0]+a[1]*x[1]))**2)*psat(\"water\") \\\n",
    "        + x[1]*t.exp(a[1]*((a[0]*x[0])/(a[0]*x[0]+a[1]*x[1]))**2)*psat(\"dioxane\") \n",
    "\n",
    "# Fix the step size\n",
    "alpha = 0.001\n",
    "\n",
    "\n",
    "# Start gradient descent\n",
    "for i in range(100):  # TODO: change the termination criterion\n",
    "    loss = lse_loss(x, y, a)\n",
    "    \n",
    "    #loss = a[0]\n",
    "    loss.backward(retain_graph=True)\n",
    "    \n",
    "    # no_grad() specifies that the operations within this context are not part of the computational graph, i.e., we don't need the gradient descent algorithm itself to be differentiable with respect to x\n",
    "    with t.no_grad():\n",
    "        #t.autograd.set_detect_anomaly(True)\n",
    "        #alpha = lin_search(pi, a.grad, x, a)\n",
    "        a -= alpha * a.grad\n",
    "        # need to clear the gradient at every step, or otherwise it will accumulate...\n",
    "        #print(loss.data.numpy())\n",
    "        a.grad.zero_()\n",
    "            \n",
    "print(a.data.numpy())\n",
    "print(loss.data.numpy())\n",
    "\n",
    "\n",
    "# Check the gradient. numpy() turns the variable from a PyTorch tensor to a numpy array.\n",
    "#a.grad.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**The value of $A$ was found to be:**\n",
    "\n",
    "$[A_{12}, A_{21}] = [1.9584178, \\;\\; 1.6891867]$ \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "with the loss of $0.67019307$. \n",
    "\n",
    "The gradient descent algorithm converged to this particular local minimum and couldn't update further. \n",
    "\n",
    "\n",
    "#### Part III\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value with A = [1.9584178 1.6891867]\n",
      "------\n",
      "[28.82409953 34.64431155 36.45296627 36.86731277 36.87400692 36.74983378\n",
      " 36.39044727 35.38482398 32.94778604 27.73000205 17.47325208]\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimated Value with A =\", a.data.numpy())\n",
    "\n",
    "print ('------')\n",
    "print(t.stack([pi(x[i], a) for i in range(x.size()[0])]).detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Estimated value is determined and compared below (rounded to nearest ten):\n",
    "\n",
    "|$x_1$ | 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0 |\n",
    "|:-----|:--------|:---------|:--------|:-----|:-----|:-----|:-----|:-----|:-----|:-----|:-----|\n",
    "|$p$| 28.1 | 34.4 | 36.7 | 36.9 | 36.8 | 36.7 | 36.5 | 35.4 | 32.9 | 27.7 | 17.5 |\n",
    "|$p_{est}$| 28.8 | 34.6 | 36.5 | 36.9 | 36.9 | 36.8 | 36.4 | 35.4 | 33.0 | 27.7 | 17.5 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Model has fit the data well with the squared error of about $0.67$. The model could perform even better if we provide more data. With the data fed to the algorithm, there is a high probability that the model has overfit the data. To avoid overfitting, feeding huge datasets usually helps among several other options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Problem 2 (50 points) \n",
    "\n",
    "Solve the following problem using Bayesian Optimization:\n",
    "$$\n",
    "    \\min_{x_1, x_2} \\quad \\left(4-2.1x_1^2 + \\frac{x_1^4}{3}\\right)x_1^2 + x_1x_2 + \\left(-4 + 4x_2^2\\right)x_2^2,\n",
    "$$\n",
    "for $x_1 \\in [-3,3]$ and $x_2 \\in [-2,2]$. A tutorial on Bayesian Optimization can be found [here](https://thuijskens.github.io/2016/12/29/bayesian-optimisation/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" gp.py\n",
    "Bayesian optimisation of loss functions.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.gaussian_process as gp\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def expected_improvement(x, gaussian_process, evaluated_loss, greater_is_better=False, n_params=1):\n",
    "    \"\"\" expected_improvement\n",
    "    Expected improvement acquisition function.\n",
    "    Arguments:\n",
    "    ----------\n",
    "        x: array-like, shape = [n_samples, n_hyperparams]\n",
    "            The point for which the expected improvement needs to be computed.\n",
    "        gaussian_process: GaussianProcessRegressor object.\n",
    "            Gaussian process trained on previously evaluated hyperparameters.\n",
    "        evaluated_loss: Numpy array.\n",
    "            Numpy array that contains the values off the loss function for the previously\n",
    "            evaluated hyperparameters.\n",
    "        greater_is_better: Boolean.\n",
    "            Boolean flag that indicates whether the loss function is to be maximised or minimised.\n",
    "        n_params: int.\n",
    "            Dimension of the hyperparameter space.\n",
    "    \"\"\"\n",
    "\n",
    "    x_to_predict = x.reshape(-1, n_params)\n",
    "\n",
    "    mu, sigma = gaussian_process.predict(x_to_predict, return_std=True)\n",
    "\n",
    "    if greater_is_better:\n",
    "        loss_optimum = np.max(evaluated_loss)\n",
    "    else:\n",
    "        loss_optimum = np.min(evaluated_loss)\n",
    "\n",
    "    scaling_factor = (-1) ** (not greater_is_better)\n",
    "\n",
    "    # In case sigma equals zero\n",
    "    with np.errstate(divide='ignore'):\n",
    "        Z = scaling_factor * (mu - loss_optimum) / sigma\n",
    "        expected_improvement = scaling_factor * (mu - loss_optimum) * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        expected_improvement[sigma == 0.0] == 0.0\n",
    "\n",
    "    return -1 * expected_improvement\n",
    "\n",
    "\n",
    "def sample_next_hyperparameter(acquisition_func, gaussian_process, evaluated_loss, greater_is_better=False,\n",
    "                               bounds=(0, 10), n_restarts=25):\n",
    "    \"\"\" sample_next_hyperparameter\n",
    "    Proposes the next hyperparameter to sample the loss function for.\n",
    "    Arguments:\n",
    "    ----------\n",
    "        acquisition_func: function.\n",
    "            Acquisition function to optimise.\n",
    "        gaussian_process: GaussianProcessRegressor object.\n",
    "            Gaussian process trained on previously evaluated hyperparameters.\n",
    "        evaluated_loss: array-like, shape = [n_obs,]\n",
    "            Numpy array that contains the values off the loss function for the previously\n",
    "            evaluated hyperparameters.\n",
    "        greater_is_better: Boolean.\n",
    "            Boolean flag that indicates whether the loss function is to be maximised or minimised.\n",
    "        bounds: Tuple.\n",
    "            Bounds for the L-BFGS optimiser.\n",
    "        n_restarts: integer.\n",
    "            Number of times to run the minimiser with different starting points.\n",
    "    \"\"\"\n",
    "    best_x = None\n",
    "    best_acquisition_value = 1\n",
    "    n_params = bounds.shape[0]\n",
    "\n",
    "    for starting_point in np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_restarts, n_params)):\n",
    "\n",
    "        res = minimize(fun=acquisition_func,\n",
    "                       x0=starting_point.reshape(1, -1),\n",
    "                       bounds=bounds,\n",
    "                       method='L-BFGS-B',\n",
    "                       args=(gaussian_process, evaluated_loss, greater_is_better, n_params))\n",
    "\n",
    "        if res.fun < best_acquisition_value:\n",
    "            best_acquisition_value = res.fun\n",
    "            best_x = res.x\n",
    "\n",
    "    return best_x\n",
    "\n",
    "\n",
    "def bayesian_optimisation(n_iters, sample_loss, bounds, x0=None, n_pre_samples=5,\n",
    "                          gp_params=None, random_search=False, alpha=1e-5, epsilon=1e-7):\n",
    "    \"\"\" bayesian_optimisation\n",
    "    Uses Gaussian Processes to optimise the loss function `sample_loss`.\n",
    "    Arguments:\n",
    "    ----------\n",
    "        n_iters: integer.\n",
    "            Number of iterations to run the search algorithm.\n",
    "        sample_loss: function.\n",
    "            Function to be optimised.\n",
    "        bounds: array-like, shape = [n_params, 2].\n",
    "            Lower and upper bounds on the parameters of the function `sample_loss`.\n",
    "        x0: array-like, shape = [n_pre_samples, n_params].\n",
    "            Array of initial points to sample the loss function for. If None, randomly\n",
    "            samples from the loss function.\n",
    "        n_pre_samples: integer.\n",
    "            If x0 is None, samples `n_pre_samples` initial points from the loss function.\n",
    "        gp_params: dictionary.\n",
    "            Dictionary of parameters to pass on to the underlying Gaussian Process.\n",
    "        random_search: integer.\n",
    "            Flag that indicates whether to perform random search or L-BFGS-B optimisation\n",
    "            over the acquisition function.\n",
    "        alpha: double.\n",
    "            Variance of the error term of the GP.\n",
    "        epsilon: double.\n",
    "            Precision tolerance for floats.\n",
    "    \"\"\"\n",
    "\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "\n",
    "    n_params = bounds.shape[0]\n",
    "\n",
    "    if x0 is None:\n",
    "        for params in np.random.uniform(bounds[:, 0], bounds[:, 1], (n_pre_samples, bounds.shape[0])):\n",
    "            x_list.append(params)\n",
    "            y_list.append(sample_loss(params))\n",
    "    else:\n",
    "        for params in x0:\n",
    "            x_list.append(params)\n",
    "            y_list.append(sample_loss(params))\n",
    "\n",
    "    xp = np.array(x_list)\n",
    "    yp = np.array(y_list)\n",
    "\n",
    "    # Create the GP\n",
    "    if gp_params is not None:\n",
    "        model = gp.GaussianProcessRegressor(**gp_params)\n",
    "    else:\n",
    "        kernel = gp.kernels.Matern()\n",
    "        model = gp.GaussianProcessRegressor(kernel=kernel,\n",
    "                                            alpha=alpha,\n",
    "                                            n_restarts_optimizer=10,\n",
    "                                            normalize_y=True)\n",
    "\n",
    "    for n in range(n_iters):\n",
    "\n",
    "        model.fit(xp, yp)\n",
    "\n",
    "        # Sample next hyperparameter\n",
    "        if random_search:\n",
    "            x_random = np.random.uniform(bounds[:, 0], bounds[:, 1], size=(random_search, n_params))\n",
    "            ei = -1 * expected_improvement(x_random, model, yp, greater_is_better=True, n_params=n_params)\n",
    "            next_sample = x_random[np.argmax(ei), :]\n",
    "        else:\n",
    "            next_sample = sample_next_hyperparameter(expected_improvement, model, yp, greater_is_better=True, bounds=bounds, n_restarts=100)\n",
    "\n",
    "        # Duplicates will break the GP. In case of a duplicate, we will randomly sample a next query point.\n",
    "        if np.any(np.abs(next_sample - xp) <= epsilon):\n",
    "            next_sample = np.random.uniform(bounds[:, 0], bounds[:, 1], bounds.shape[0])\n",
    "\n",
    "        # Sample loss for new set of parameters\n",
    "        cv_score = sample_loss(next_sample)\n",
    "\n",
    "        # Update lists\n",
    "        x_list.append(next_sample)\n",
    "        y_list.append(cv_score)\n",
    "\n",
    "        # Update xp and yp\n",
    "        xp = np.array(x_list)\n",
    "        yp = np.array(y_list)\n",
    "\n",
    "    return xp, yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def obj_f(x):\n",
    "    return (4 - 2.1*x[0]**2 + (x[0]**4)/3)*x[0]**2 + x[0]*x[1] + (4*x[1]**2 - 4)*x[1]**2\n",
    "\n",
    "\n",
    "x, y = bayesian_optimisation(1000, obj_f, bounds = np.array([[-3, 3],[-2, 2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.argmin(), y.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_f(x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x1 = np.linspace(-3, 3, 100)\n",
    "x2 = np.linspace(-2, 2, 100)\n",
    "\n",
    "x = list(map(list, zip(x1, x2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = []\n",
    "for i in range(len(x)):\n",
    "    z.append(obj_f(x[i]))\n",
    "    \n",
    "z = np.array(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib qt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(10.5, 10.5)\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_zlabel('f')\n",
    "\n",
    "\n",
    "\n",
    "# Make data.\n",
    "X, Y = np.meshgrid(x1, x2)\n",
    "\n",
    "z = (4 - 2.1*X**2 + (X**4)/3)*X**2 + X*Y + (4*Y**2 - 4)*X*Y**2\n",
    "\n",
    "# Plot the surface.\n",
    "ax.plot_surface(X, Y, z, cmap=cm.viridis,\n",
    "                       linewidth=0, antialiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
