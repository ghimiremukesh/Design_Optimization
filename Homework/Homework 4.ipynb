{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 (10 Points)\n",
    "\n",
    "Sketch graphically the problem \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{x_1,x_2} & \\quad f({\\bf x})=(x_1+1)^2+(x_2-2)^2\\\\\n",
    "{\\text{subject to }} & \\quad g_1 = x_1-2\\leq 0,{\\quad} g_3 = -x_1\\leq 0,\\\\\n",
    "& \\quad g_2 = x_2-1\\leq 0, {\\quad} g_4 = -x_2\\leq 0.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Find the optimum graphically. Determine directions of feasible descent at the corner points of the feasible domain. Show the gradient directions of $f$ and $g_i$s at these points. Verify graphical results analytically using the KKT conditions.\n",
    "\n",
    "---\n",
    "### Solution\n",
    "---\n",
    "\n",
    "First we plot the ojective function and the constraints. For the objective function, we plot its contour lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = np.linspace(-1, 2, 1000)\n",
    "Y = np.linspace(-1, 2, 1000)\n",
    "\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "Z = (X+1)**2 + (Y-2)**2\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 6))\n",
    "cp = plt.contour(X, Y, Z, levels= [i for i in range(0, 20, 2)])\n",
    "#plt.clabel(cp, inline=1, fontsize=10)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "\n",
    "\n",
    "\n",
    "y = np.linspace(-1,2.5)\n",
    "plt.plot(2*np.ones(len(y)), y, c='red')\n",
    "plt.plot(y, np.ones(len(y)), c = 'red')\n",
    "plt.plot(np.zeros(len(y)), y, c='red')\n",
    "plt.plot(y, np.zeros(len(y)), c='red')\n",
    "plt.ylim([-1, 1.5])\n",
    "\n",
    "\n",
    "plt.clabel(cp, inline=True, fontsize=8)\n",
    "plt.title(\"Contour of the objective function w/constraints\")  \n",
    "plt.scatter(2, 0, marker='x', s=80, color='blue', label='maximum')\n",
    "plt.scatter(0,1, marker='o', s=80, color='magenta', label='minimum')\n",
    "\n",
    "x = np.linspace(0, 2, 100)\n",
    "y = np.linspace(0, 1, 100)\n",
    "plt.fill_between(x, y, 0, color='green', label='feasible-region')\n",
    "plt.fill_between(x, y, 1, color='green')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we can see that the minimizer is $(x_1, x_2) = (0, 1)$ and the maximizer is $(x_1, x_2) = (2, 0)$\n",
    "\n",
    "### Direction of feasible descent at the corner points: \n",
    "\n",
    "To determine the direction of feasible descents at the corner points we take the gradient of the ojective function first and evaluate at the corner points. Then we find the gradient of the activate constraints at the same points.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\quad\\nabla f({\\bf x})=\\begin{bmatrix} 2x_1 + 2 \\\\\n",
    "                                       2x_2 -4 \\\\\n",
    "                                       \\end{bmatrix}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The gradients of the constraints are:\n",
    "\n",
    "So, \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\quad\\nabla g_1 &=\\begin{bmatrix} 1 \\\\\n",
    "                                 0 \\\\\n",
    "                                       \\end{bmatrix}\\\\  \n",
    "\\quad\\nabla g_2 &=\\begin{bmatrix} 0 \\\\\n",
    "                                 1 \\\\\n",
    "                                       \\end{bmatrix}\\\\                      \n",
    "\\quad\\nabla g_3 &=\\begin{bmatrix} -1 \\\\\n",
    "                                 0 \\\\\n",
    "                                       \\end{bmatrix}\\\\\n",
    "\\quad\\nabla g_4 &=\\begin{bmatrix} 0 \\\\\n",
    "                                 -1 \\\\\n",
    "                                       \\end{bmatrix}\\\\  \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "The active constraints at the minimizer $(0, 1)$ are: $g_2\\;\\text{and}\\;g_3$ and the gradient of the objective function is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\quad\\nabla f({\\bf x})=\\begin{bmatrix} 2 \\\\\n",
    "                                       -2 \\\\\n",
    "                                       \\end{bmatrix}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Similarly, the active constraints at the maximizer $(2, 0)$ are: $g_1\\;\\text{and}\\;g_4$ and the gradient of the objective function is:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\quad\\nabla f({\\bf x})=\\begin{bmatrix} 6 \\\\\n",
    "                                       -4 \\\\\n",
    "                                       \\end{bmatrix}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Plotting the above gradients in the above plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = np.linspace(-1, 2, 1000)\n",
    "Y = np.linspace(-1, 2, 1000)\n",
    "\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "Z = (X+1)**2 + (Y-2)**2\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 6))\n",
    "cp = plt.contour(X, Y, Z, levels= [i for i in range(0, 20, 2)])f\n",
    "#plt.clabel(cp, inline=1, fontsize=10)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "\n",
    "\n",
    "\n",
    "y = np.linspace(-1,2.5)\n",
    "plt.plot(2*np.ones(len(y)), y, c='brown', linestyle='dashed')\n",
    "plt.plot(y, np.ones(len(y)), c='brown', linestyle='dashed')\n",
    "plt.plot(np.zeros(len(y)), y, c='brown', linestyle='dashed')\n",
    "plt.plot(y, np.zeros(len(y)), c='brown', linestyle='dashed')\n",
    "plt.ylim([-1, 1.5])\n",
    "\n",
    "\n",
    "plt.clabel(cp, inline=True, fontsize=8)\n",
    "plt.title(\"Gradient direction of the objective function and contraints at the optimum points\")  \n",
    "plt.scatter(2, 0, marker='x', s=200, color='blue', label='maximum')\n",
    "plt.scatter(0,1, marker='o', s=200, color='magenta', label='minimum')\n",
    "\n",
    "x = np.linspace(0, 2, 100)\n",
    "y = np.linspace(0, 1, 100)\n",
    "plt.fill_between(x, y, 0, color='green', label='feasible-region')\n",
    "plt.fill_between(x, y, 1, color='green')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "origin1 = np.array([0, 1])\n",
    "plt.quiver(*origin1, [1, 0, -1], [-1, 1 ,0], color=['black', 'red', 'green'], scale=10)\n",
    "\n",
    "origin2 = np.array([2, 0])\n",
    "plt.quiver(*origin2, [1.5, 1, 0], [-1, 0 ,-1], color=['black', 'red', 'green'], scale=10)\n",
    "\n",
    "origin3 = np.array([0, 0])\n",
    "plt.quiver(*origin3, [1, 1, 0], [-2, 0 ,-1], color=['black', 'red', 'green'], scale=10)\n",
    "\n",
    "origin4 = np.array([2, 1])\n",
    "plt.quiver(*origin4, [1.5, 1, 0], [-0.5, 0 ,-1], color=['black', 'green', 'red'], scale=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------\n",
    "\n",
    "**The optimum value is $f = 2$ at $(x_1, x_2) = (0, 1)$**\n",
    "\n",
    "----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical Solution Using KKT conditions:\n",
    "---\n",
    "\n",
    "For KKT conditions, we need Lagrangian, which is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\quad \\mathcal{L} =\\quad f + \\mu_1 g_1 + \\mu_2 g_2 + \\mu_3 g_3 + \\mu_4 g_4 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where, $\\mu_i$s are the dual variables (aka lagrange multipliers) for the inequality constraints $g_i$s.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\quad \\mathcal{L} = (x_1+1)^2+(x_2-2)^2 + \\mu_1 (x_1-2) + \\mu_2 (x_2-1) + \\mu_3 (-x_1) + \\mu_4 (-x_4) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then, the gradient of the Lagrangian is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\quad \\nabla \\mathcal{L} = \\begin{bmatrix} 2x_1 + 2 + \\mu_1 - \\mu_3\\\\\n",
    "                                       2x_2 -4 + \\mu_2 - \\mu_4 \\\\\n",
    "                                       \\end{bmatrix}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Set $\\nabla \\mathcal{L} = 0$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{bmatrix} 2x_1 + 2 + \\mu_1 - \\mu_3\\\\\n",
    "                                       2x_2 -4 + \\mu_2 - \\mu_4 \\\\\n",
    "                                       \\end{bmatrix} &= \\begin{bmatrix} 0 \\\\\n",
    "                                                                        0 \\end{bmatrix}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "Then, at the minimum, taking $g_2$ and $g_3$ as activate constraints (usually we can guess when we don't have any representation of the objective function). Then, using the matrix above along with the active constraints, we can solve for $\\mu_i$s (note: $\\mu_1 = \\mu_4 = 0$):\n",
    "\n",
    "We get $\\mu_2 = 2$, $\\mu_3 = 2$, $x_1 = 0$ and $x_2 = 1$. \n",
    "\n",
    "As $\\mu_2 > 0$ and $\\mu_3 > 0$, and the feasible region is convex, <b> the minimum occurs at (0, 1) and the minimum value is 2. </b>\n",
    "\n",
    "#### Hence, the result is consistent with the graphical method.\n",
    "\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 (10 Points)\n",
    "\n",
    "Graph the problem \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{x_1,x_2} & \\quad  f=-x_1\\\\\n",
    "{\\text{subject to }} & \\quad g_1=x_2-(1-x_1)^3\\leq 0{\\quad} {\\rm and}{\\quad} x_2\\geq 0.\n",
    "\\end{aligned}\n",
    "$$ \n",
    "\n",
    "Find the solution graphically. Then apply the optimality conditions. Can you find a solution based on the optimality conditions? Why? (From Kuhn and Tucker, 1951.)\n",
    "\n",
    "---\n",
    "Solution: Graphically\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1.4, 100)\n",
    "y = np.linspace(-1, 2, 100)\n",
    "g = (1-x)**3\n",
    "\n",
    "\n",
    "X = np.linspace(-1, 1.4, 100)\n",
    "Y = np.linspace(-1,8,100)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "Z = -X\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 6))\n",
    "cp = plt.contour(X, Y, Z)\n",
    "\n",
    "plt.plot(x, g, label = 'g1')\n",
    "plt.plot(x, np.zeros(len(x)), linestyle='dashed')\n",
    "\n",
    "plt.fill_between(x, g, 0, color='yellow', label='feasible-region')\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend()\n",
    "plt.title(\"Objective function w/constraints and feasible region\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we can observe that the minimum value for the given constraints occurs at $\\mathbf{(x_1, x_2) = (1, 0)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution using optimality condition\n",
    "\n",
    "The Lagrangian of the optimization problem is: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\quad \\mathcal{L} &=\\quad f + \\mu_1 g_1 + \\mu_2 (-x_2) \\\\\n",
    "                  &=\\quad -x_1 + \\mu_1 (x_2-(1-x_1)^3)  + \\mu_2 (-x_2) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then, the gradient of the Lagrangian is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\quad \\nabla \\mathcal{L} = \\begin{bmatrix} 3\\mu_1(1 - x_1)^2 -1\\\\\n",
    "                                       \\mu_1 - \\mu_2 \\\\\n",
    "                                       \\end{bmatrix}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Set $\\nabla \\mathcal{L} = 0$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{bmatrix} 3\\mu_1(1 - x_1)^2 -1\\\\\n",
    "                                       \\mu_1 - \\mu_2 \\\\\n",
    "                                       \\end{bmatrix} &= \\begin{bmatrix} 0 \\\\\n",
    "                                                                        0 \\end{bmatrix}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set $g_2$ as active constraint, then $\\mu_1 = 0, \\mu_2 > 0$. However, $\\mu_2 = 0$ from the above relationship. \n",
    "\n",
    "\n",
    "We can determine the gradient of the objective function and constraints at (1,0). \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\quad \\nabla f_{(1, 0)} = \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix}\\\\\n",
    "\\quad \\nabla g_{1 (1, 0)} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\\\\n",
    "\\quad \\nabla g_{2 (1,0)} = \\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can see that the gradient of the constraints are linearly dependent, and hence we cannot determine the optimal solution using the optimality condition. We can plot these vectors to see their behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1.4, 100)\n",
    "y = np.linspace(0, 2, 100)\n",
    "g = (1-x)**3\n",
    "\n",
    "\n",
    "X = np.linspace(-1, 1.4, 100)\n",
    "Y = np.linspace(-1,8,100)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "Z = -X\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 6))\n",
    "cp = plt.contour(X, Y, Z)\n",
    "\n",
    "plt.plot(x, g, label = 'g1')\n",
    "plt.plot(x, np.zeros(len(x)), linestyle='dashed')\n",
    "\n",
    "plt.fill_between(x, g, 0, color='yellow', label='feasible-region')\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend()\n",
    "plt.title(\"Objective function w/constraints and feasible region\")\n",
    "origin1 = np.array([1, 0])\n",
    "plt.quiver(*origin1, [-1, 0, 0], [0, 1 ,-1], color=['black', 'red', 'green'], scale=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 (30 Points)\n",
    "\n",
    "Find a local solution to the problem \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_{x_1,x_2,x_3} & \\quad  f=x_1x_2+x_2x_3+x_1x_3\\\\\n",
    "{\\text{subject to }} & \\quad h=x_1+x_2+x_3-3=0.\n",
    "\\end{aligned}\n",
    "$$ \n",
    "\n",
    "Use two methods: reduced gradient and Lagrange multipliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1: Using Reduced Gradient\n",
    "---\n",
    "\n",
    "In reduced gradient method, first, we need to identify the number of constraints (m). Here, m = 1.\n",
    "\n",
    "So, there must be n - m decision variables, where n = 3. Hence, $d \\in \\mathbb{R}^2$ and $s \\in \\mathbb{R}^1$\n",
    "\n",
    "We pick $x_1$ as state variable and $x_2, x_3$ as decision variables. Then: \n",
    "\n",
    "$x = \\begin{bmatrix}d \\\\ s\\end{bmatrix}$\n",
    "\n",
    "Now, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial f}{\\partial d} &= \\begin{bmatrix} -x_1 - x_3 \\\\ -x_1 - x_2\\end{bmatrix}\\\\\n",
    "\\frac{\\partial f}{\\partial s} &= -x_2 - x_3 \\\\\n",
    "\\frac{\\partial h}{\\partial d} &= \\begin{bmatrix} 1 \\\\ 1\\end{bmatrix}\\\\\n",
    "\\frac{\\partial h}{\\partial s} &= 1\\\\ \\\\\n",
    "\\text{And,}\\\\\n",
    "\\frac{df}{dd} &= \\frac{\\partial f}{\\partial d} - \\left(\\frac{\\partial f}{\\partial s}\\right)\\left(\\frac{\\partial h}{\\partial s} \\right)^{-1} \\frac{\\partial h}{\\partial d}\\\\\n",
    "              &= \\begin{bmatrix} -x_1 - x_3 \\\\ -x_1 - x_2\\end{bmatrix} - (-x_2 - x_3) (1) \\begin{bmatrix} 1 \\\\ 1\\end{bmatrix}\\\\\n",
    "              &= \\begin{bmatrix} -x_1 - x_3 \\\\ -x_1 - x_2\\end{bmatrix} - \\begin{bmatrix} -x_2 - x_3 \\\\ -x_2 - x_3\\end{bmatrix}\\\\\n",
    "              &= \\begin{bmatrix} x_2 - x_1 \\\\ x_3 - x_1\\end{bmatrix}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Set $\\frac{df}{dd} = 0$. We get: $x_1 = x_2 = x_3$. Plugging these back into the constraints results in:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_1^* = x_2^* = x_3^* = 1 \\;\\;\\text{and}\\;\\; f^* = 3\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "<!-- To verify that the above solutions are indeed local minimum we need to check the second order sufficient condition.  \n",
    "\n",
    "\n",
    "We know, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\quad \\frac{{\\rm d}s}{{\\rm d}d} &= -\\left(\\frac{\\partial h}{\\partial s}\\right)^{-1} \\frac{\\partial h}{\\partial d} \\\\ \n",
    "& = \\begin{bmatrix} -1 \\\\ -1 \\end{bmatrix}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then, We can see, $\\frac{{\\rm d^2} s}{{\\rm d} d^2} = 0$.\n",
    "\n",
    "Now,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\quad \\frac{{\\rm d}^2 f}{{\\rm d}d^2} &= \n",
    "\\begin{bmatrix}\n",
    "I & \\left(\\frac{{\\rm d}s}{{\\rm d}d}\\right)^T\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{{\\partial}^2 f}{{\\partial}d^2} & \\frac{{\\partial}f}{{\\partial}d{\\partial}s}\\\\\n",
    "\\frac{{\\partial}f}{{\\partial}s{\\partial}d} & \\frac{{\\partial}^2 f}{{\\partial}s^2}\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "I \\\\\n",
    "\\frac{{\\rm d}s}{{\\rm d}d}\\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\frac{{\\partial}f}{{\\partial}s}\\frac{{\\rm d}^2 s}{{\\partial}d^2} \\\\ \n",
    "& =\n",
    "\\begin{bmatrix}\n",
    "1&-1&-1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0&1&1 \\\\\n",
    "1&0&1 \\\\\n",
    "1&1&0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "-1 \\\\\n",
    "-1 \\\\\n",
    "\\end{bmatrix} = -2 < 0\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Its not the minimizer.  -->\n",
    "\n",
    "\n",
    "<!-- $$\n",
    "\\begin{aligned}\n",
    "\\quad & \\frac{\\rm d^2 h}{\\rm d d^2} = \n",
    "\\begin{bmatrix}\n",
    "I&\\left(\\frac{{\\rm d}s}{{\\rm d}d}\\right)^T\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{{\\partial}^2 h}{{\\partial}d^2} & \\frac{{\\partial}^2h}{{\\partial}d{\\partial}s}\\\\\n",
    "\\frac{{\\partial}^2h}{{\\partial}s{\\partial}d} & \\frac{{\\partial}^2 h}{{\\partial}s^2}\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "I \\\\\n",
    "\\frac{{\\rm d}s}{{\\rm d}d}\\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\frac{{\\partial}h}{{\\partial}s}\\frac{{\\rm d}^2 s}{{\\rm d}d^2} = 0\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Then, \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\quad & \\frac{{\\partial}^2 f}{{\\partial}d^2} = \n",
    "\\begin{bmatrix}\n",
    "0 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\quad & \\frac{{\\partial}^2 f}{{\\partial}s^2} = 0\\\\\n",
    "\\quad & \\frac{{\\partial}f}{{\\partial}s{\\partial}d} = \n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\quad & \\frac{{\\partial}f}{{\\partial}d{\\partial}s} = \n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\end{aligned}\n",
    "$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Solution 2: Using Lagrange Multipliers\n",
    "\n",
    "\n",
    "The Lagrangian for the optimization problem is $\\mathcal{L} = -f + \\lambda_1 h$, where $h$ is the equality constraint. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\quad \\mathcal{L} &= -x_1x_2 - x_2x_3 - x_1x_3 + \\lambda_1 (x_1 + x_2 + x_3 - 3)\\\\\n",
    "\\text{Then, gradient of the Lagrangian is:}\\\\\n",
    "\\quad \\nabla \\mathcal{L} &= \\begin{bmatrix} -x_2 - x_3 + \\lambda_1 \\\\\n",
    "                                            -x_1 - x_2 + \\lambda_1 \\\\\n",
    "                                            -x_1 - x_3 + \\lambda_1\n",
    "\\end{bmatrix} = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "On solving, we get:\n",
    "\n",
    "$x_1 = x_2 = x_3 = 1, \\lambda = 2$.\n",
    "\n",
    "To further verify the optimality, we need to check for second order sufficiency condition.\n",
    "\n",
    "<!-- $$\n",
    "\\begin{aligned}\n",
    "\\quad \\frac{{\\rm d}^2\\mathcal{L}}{{\\rm d}x^2} &= \\begin{bmatrix} 0 & 1 & 1 & 1\\\\\n",
    "                                                                 1 & 0 & 1 & 1\\\\\n",
    "                                                                 1 & 1 & 0 & 1\\\\\n",
    "                                                                 1 & 1 & 1 & 0\\\\\\end{bmatrix}\\;\\;\\text{p.d}\n",
    "\\end{aligned}\n",
    "$$ -->\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\partial x^T\\mathcal{L_{xx}}\\partial x &= \\begin{bmatrix}dx_1 & dx_2 & dx_3 \\end{bmatrix} \\begin{bmatrix} 0 & 1 & 1\\\\\n",
    "                                                                 1 & 0 & 1\\\\\n",
    "                                                                 1 & 1 & 0\\\\\n",
    "                                                                 \\end{bmatrix} \\begin{bmatrix}dx_1 \\\\ dx_2 \\\\ dx_3 \n",
    "                                                                 \\end{bmatrix}\n",
    "                                        &= -(dx_1^2 +dx_2^2 - dx_1dx_2)\\\\\n",
    "                                        &= -\\left(\\left(dx_1 - \\frac{dx_2}{2}\\right)^2 + \\frac{4}{3}dx_2^2\\right) \\le 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Hence, $x_1^* = x_2^* = x_3^* = 1$ is the global maximizer. \n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4 (20 Points)\n",
    "\n",
    "Use reduced gradient to\tfind the value(s) of the parameter $b$ for which the point $x_1=1$, $x_2=2$ is the solution to the problem \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_{x_1,x_2} & \\quad  f=2x_{1} + bx_2\\\\\n",
    "{\\text{subject to }} & \\quad g_1 = x_{1}^{2}+ x_{2}^{2}-5\\leq 0\\\\\n",
    "& \\quad g_2= x_1- x_2-2\\leq 0.\n",
    "\\end{aligned}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: \n",
    "\n",
    "At $x_1 = 1$ and $x_2 = 2$, set $g_1$ as the active constraint. Then, assume $x_1$ is the state variable and $x_2$ is the decision variable. Then, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\quad \\frac{\\partial f}{\\partial d} &= b \\\\\n",
    "\\quad \\frac{\\partial f}{\\partial s} &= 2 \\\\\n",
    "\\quad \\frac{\\partial h}{\\partial d} &=  \\frac{\\partial g_1}{\\partial d} = 2x_2 \\\\\n",
    "\\quad \\frac{\\partial h}{\\partial s} &= \\frac{\\partial g_1}{\\partial s} = 2x_1 \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{df}{dd} &= \\frac{\\partial f}{\\partial d} - \\left(\\frac{\\partial f}{\\partial s}\\right)\\left(\\frac{\\partial h}{\\partial s} \\right)^{-1} \\frac{\\partial h}{\\partial d}\\\\\n",
    "              &= b - 2 \\left(\\frac{1}{2x_1}\\right)2x_2 \\\\\n",
    "              &= b - \\frac{2x_2}{x_1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Then, at $x_1 = 1$ and $x_2 = 2$, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{df}{dd} &= b - \\frac{2\\times2}{1} = 0 \\\\\n",
    "\\text{Hence,}\\\\\n",
    "              b &= 4\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the second order sufficient condition as well. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\quad \\frac{{\\rm d}^2 f}{{\\rm d}d^2} &= \n",
    "\\begin{bmatrix}\n",
    "I & \\left(\\frac{{\\rm d}s}{{\\rm d}d}\\right)^T\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{{\\partial}^2 f}{{\\partial}d^2} & \\frac{{\\partial}f}{{\\partial}d{\\partial}s}\\\\\n",
    "\\frac{{\\partial}f}{{\\partial}s{\\partial}d} & \\frac{{\\partial}^2 f}{{\\partial}s^2}\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "I \\\\\n",
    "\\frac{{\\rm d}s}{{\\rm d}d}\\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\frac{{\\partial}f}{{\\partial}s}\\frac{{\\rm d}^2 s}{{\\rm d}d^2} \\\\ \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\quad \\frac{{\\rm d}s}{{\\rm d}d} &= -\\left(\\frac{\\partial h}{\\partial s}\\right)^{-1} \\frac{\\partial h}{\\partial d} \\\\ \n",
    "& = -\\frac{1}{2x_1} 2x_2\\\\\n",
    "& = -\\frac{x_2}{x_1}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\quad \\frac{{\\rm d}^2 h}{{\\rm d}d^2} &= \n",
    "\\begin{bmatrix}\n",
    "I & \\left(\\frac{{\\rm d}s}{{\\rm d}d}\\right)^T\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{{\\partial}^2 h}{{\\partial}d^2} & \\frac{{\\partial}h}{{\\partial}d{\\partial}s}\\\\\n",
    "\\frac{{\\partial}h}{{\\partial}s{\\partial}d} & \\frac{{\\partial}^2 h}{{\\partial}s^2}\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "I \\\\\n",
    "\\frac{{\\rm d}s}{{\\rm d}d}\\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\frac{{\\partial}h}{{\\partial}s}\\frac{{\\rm d}^2 s}{{\\rm d}d^2} = 0 \\\\ \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "1 & -2\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "2 & 0\\\\\n",
    "0 & 2\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "-2\\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "2\\frac{{\\rm d}^2 s}{{\\rm d}d^2} = 0 \\\\ \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{{\\rm d}^2 s}{{\\rm d}d^2} &=  -5\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\quad \\frac{{\\rm d}^2 f}{{\\rm d}d^2} &= \\frac{{\\partial}f}{{\\partial}s}\\frac{{\\rm d}^2 s}{{\\partial}d^2}\\;\\;\\;\\left(\\text{Since}\\;\\;f_{xx} = 0 \\right)\n",
    "&= 2 (-5) = -10 \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore, b=4 maximizes the objective and the maximum value is: 2 \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Problem 5 (30 Points)\n",
    "\n",
    "Find the solution for \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{x_1,x_2,x_3} & \\quad  f=x_{1}^{2}+x_{2}^{2}+x_{3}^{2}\\\\\n",
    "{\\text{subject to }} & \\quad h_1 = x_{1}^{2}/4+x_{2}^{2}/5+x_{3}^{2}/25-1=0\\\\\n",
    "& \\quad h_2 = x_1+x_2-x_3= 0,\n",
    "\\end{aligned}\n",
    "$$ \n",
    "\n",
    "by implementing the generalized reduced gradient algorithm.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### Solution\n",
    "\n",
    "For generalized reduced gradient alogirthm, we need to separate the space into state and decision spaces. Since there are 2 constraints, we need to state variables and one decision variables. We pick $x_1$ as decision variable and $x_2$ and $x_3$ as state variables. Following program is used to calculate the reduced gradient using the GRG algorithm paired with the line search algorithm to find the optimum step size at each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "\n",
    "# objective function\n",
    "f = lambda x: x[0][0]**2 + x[1][0]**2 + x[2][0]**2\n",
    "\n",
    "# constraints \n",
    "def f_g(x):\n",
    "    return np.array([[2*x[0][0]],[2*x[1][0]],[2*x[2][0]]]);\n",
    "\n",
    "\n",
    "# m = 2, n = 3, n-m = 1; \n",
    "\n",
    "def const(x):\n",
    "    return np.array([[(x[0][0]**2)/4 + (x[1][0]**2)/5 + (x[2][0]**2)/25 - 1], [x[0][0] + x[1][0] - x[2][0]]])\n",
    "\n",
    "def constg(x):\n",
    "    return np.array([[x[0][0]/2, 2*x[1][0]/5, 2*x[2][0]/25],[1, 1, -1]])\n",
    "\n",
    "\n",
    "# decision var -- x1, x2 & x3 are state\n",
    "def solveh(x, const, constg):\n",
    "    import matplotlib.pyplot as plt\n",
    "    error = 1e-2\n",
    "    \n",
    "    s = x[1:] # state \n",
    "    \n",
    "    #check if the constraint is satisfied\n",
    "    \n",
    "    h = const(x)\n",
    "     # take pseudo inverse\n",
    "    norm = []\n",
    "    count = 0\n",
    "    while np.linalg.norm(h) >= error and count < 20:\n",
    "        count += 1\n",
    "        dh_dx =  constg(x)\n",
    "        dh_ds = dh_dx[:,1:]\n",
    "        dh_inv = np.linalg.pinv(dh_ds)\n",
    "        s = s - np.matmul(dh_inv, h)\n",
    "        x[1] = s[0][0]\n",
    "        x[2] = s[1][0]\n",
    "        h = const(x)\n",
    "        norm.append(np.linalg.norm(h))\n",
    "    \n",
    "    #plt.plot(norm)\n",
    "    return x\n",
    "    \n",
    "\n",
    "def df_dd(x):\n",
    "    par_fd = f_g(x)[0].reshape(1,1)\n",
    "    par_fs = f_g(x)[1:]\n",
    "    dh_dx =  constg(x)\n",
    "    dh_ds = dh_dx[:,1:]\n",
    "    dh_dd = dh_dx[:, 0].reshape(2,1)\n",
    "    dh_inv = np.linalg.inv(dh_ds)\n",
    "\n",
    "    \n",
    "    #df_dd = f_g(x)[0] - np.matmul(np.matmul(f_g(x)[1:, :].T, dh_inv), dh_dx[:,0].reshape(2,1))\n",
    "    \n",
    "    df_dd1 = par_fd - np.matmul(par_fs.T, np.matmul(dh_inv, dh_dd))\n",
    "    \n",
    "    return df_dd1, dh_inv, dh_ds, dh_dd\n",
    "    \n",
    "def line_search(x):\n",
    "    a = 1. \n",
    "    tilt = 0.3\n",
    "    dfdd, _, _, _ = df_dd(x)\n",
    "    phi = lambda x, a, tilt, dfdd: f(x) - a * tilt * np.matmul(dfdd, dfdd.T)\n",
    "\n",
    "    def f_a(x, alpha, tilt):\n",
    "        dfdd, dh_inv, dh_ds, dh_dd = df_dd(x)\n",
    "        \n",
    "        x[0][0] = x[0][0] - alpha*dfdd\n",
    "        x[1:] = x[1:] + alpha*np.matmul(np.matmul(np.linalg.inv(dh_ds), dh_dd), dfdd.T)\n",
    "        return f(x) \n",
    "                                     \n",
    "                                      \n",
    "    while phi(x, a, tilt, dfdd) < f_a(x, a, tilt):\n",
    "        a *= 0.5\n",
    "        dfdd, _, _, _ = df_dd(x)\n",
    "    return a   \n",
    "\n",
    "    \n",
    "def GRG(x, const, constg):\n",
    "    \n",
    "    error = 1e-3\n",
    "    \n",
    "    df_dd1, dh_inv, dh_ds,_ = df_dd(x)\n",
    "    h = const(x)\n",
    "    count = 0\n",
    "    norms = []\n",
    "    I = np.diag(np.ones((dh_ds.shape[0])))\n",
    "    lam = 1\n",
    "    xs = []\n",
    "    while (np.linalg.norm(df_dd1) >= error):\n",
    "        x1 = x.copy() # don't want to change the initial x --> pass by reference\n",
    "        alpha = line_search(x1)\n",
    "        norms.append(np.linalg.norm(df_dd1))\n",
    "        count += 1\n",
    "        x[0][0] -= alpha * df_dd1\n",
    "        s = x[1:]\n",
    "\n",
    "        s = s - np.matmul(np.linalg.inv(np.matmul(dh_ds.T, dh_ds)+lam*I), np.matmul(dh_ds.T, h))\n",
    "                \n",
    "        x[1:] = s\n",
    "        x = solveh(x, const, constg)\n",
    "        \n",
    "        df_dd1, dh_inv, dh_ds,_ = df_dd(x)\n",
    "        h = const(x)\n",
    "        xs.append(x)\n",
    "        \n",
    "    return x, norms, xs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial guess\n",
    "x = np.array([[1.,2.,3.]]).T\n",
    "\n",
    "x_min, norms, xs = GRG(x, const, constg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5753846 ],\n",
       "       [ 1.37773504],\n",
       "       [-0.19693537]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Hence, the minimizer for the given problem is: $\\left(x_1^*, x_2^*, x_3^*\\right) = \\left(-1.5754, 1.37773, -0.19694\\right)$</b> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
